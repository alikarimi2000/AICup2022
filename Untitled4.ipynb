{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1m_0RH6t4DjLQws7dIHu8Ly_fpVgoeyJD",
      "authorship_tag": "ABX9TyOyeJpcj0sZq3CjANxR6359",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alikarimi2000/AIcup2022/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp /content/drive/MyDrive/Colab\\ Notebooks/model_utils.py /content\n",
        "!cp /content/drive/MyDrive/Colab\\ Notebooks/train.py /content\n",
        "!cp /content/drive/MyDrive/Colab\\ Notebooks/predict_utils.py /content\n",
        "!cp /content/drive/MyDrive/Colab\\ Notebooks/vis_utils.py /content\n",
        "!cp /content/drive/MyDrive/Colab\\ Notebooks/build_vocab.py /content\n",
        "!cp /content/drive/MyDrive/Colab\\ Notebooks/data_loader.py /content"
      ],
      "metadata": {
        "id": "Pjsl1H4-zvaP"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "YG1g80OqZxCw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64520a6b-a47a-4f71-8a03-24e8fffaecc3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!cp /content/drive/MyDrive/Colab\\ Notebooks/model_utils.py /content\n",
        "!cp /content/drive/MyDrive/Colab\\ Notebooks/train.py /content\n",
        "!cp /content/drive/MyDrive/Colab\\ Notebooks/predict_utils.py /content\n",
        "!cp /content/drive/MyDrive/Colab\\ Notebooks/vis_utils.py /content"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ! gdown --id 1GC7qe9xTPjPAlQmZQPMdYjROgVNAmYWD\n",
        "# ! unzip Problem03.zip"
      ],
      "metadata": {
        "id": "WSKdjYNsycX_"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "Pe9NsiXqyIDe"
      },
      "outputs": [],
      "source": [
        "%reload_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import pickle\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "from glob import glob\n",
        "from IPython.display import display\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "\n",
        "# from utils import *\n",
        "from build_vocab import build_vocab\n",
        "from data_loader import get_loader\n",
        "\n",
        "# setup\n",
        "use_gpu = torch.cuda.is_available()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "use_gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-b8gotuc3jft",
        "outputId": "5ced2d7e-3139-4747-eb8f-dec6adf52893"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_cnn_model(model_name, pretrained=True):\n",
        "    \"Load and return a convolutional neural network.\"\n",
        "    assert model_name in ['resnet18', 'resnet34', 'resnet50', 'resnet101', 'resnet152']\n",
        "    return models.__dict__[model_name](pretrained)\n",
        "\n",
        "\n",
        "def load_image(image_path, transform=None):\n",
        "    \"Load an image and perform given transformations.\"\n",
        "    image = Image.open(image_path)    \n",
        "    if transform is not None:\n",
        "        image = transform(image).unsqueeze(0)\n",
        "    return image"
      ],
      "metadata": {
        "id": "OhcdXdFUyyH1"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = build_vocab()"
      ],
      "metadata": {
        "id": "ziTT4iUKy0_9"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(16):\n",
        "    print(\"%s --> %d\" %(vocab.idx2word[i], i))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qSbY2ZgZy2rd",
        "outputId": "0406a1cc-e7c5-4acb-cca1-f6ade7fc1eb2"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<PAD> --> 0\n",
            "<BOS> --> 1\n",
            "<EOS> --> 2\n",
            "<UNK> --> 3\n",
            "K --> 4\n",
            "k --> 5\n",
            "N --> 6\n",
            "n --> 7\n",
            "p --> 8\n",
            "P --> 9\n",
            "Q --> 10\n",
            "q --> 11\n",
            "- --> 12\n",
            "B --> 13\n",
            "b --> 14\n",
            "R --> 15\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "images_dir = '/content/Problem03/train'\n",
        "image_size = 128\n",
        "crop_size  = 128\n",
        "batch_size = 16\n",
        "\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    #transforms.RandomCrop(crop_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),\n",
        "                         (0.229, 0.224, 0.225))\n",
        "])"
      ],
      "metadata": {
        "id": "blNZORqiy5bk"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_loader = get_loader(images_dir, vocab, \n",
        "                         transform, batch_size, \n",
        "                         shuffle=True, num_workers=0)"
      ],
      "metadata": {
        "id": "iyXlZMWky7d8"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs, caps, lengths = next(iter(data_loader))\n",
        "\n",
        "\n",
        "print(\" \".join([str(id) for id in caps[0][1:-1]]))\n",
        "\n",
        "print(\" \".join([vocab.idx2word[int(id)] for id in caps[0][1:-1]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HscXZUsWy9s0",
        "outputId": "49cb4c72-e9c7-49d4-8518-a21fcff6c6b0"
      },
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(16) tensor(7) tensor(14) tensor(17) tensor(5) tensor(19) tensor(12) tensor(8) tensor(18) tensor(8) tensor(17) tensor(8) tensor(11) tensor(17) tensor(12) tensor(20) tensor(8) tensor(18) tensor(16) tensor(12) tensor(17) tensor(8) tensor(18) tensor(9) tensor(19) tensor(12) tensor(18) tensor(8) tensor(9) tensor(18) tensor(7) tensor(17) tensor(12) tensor(18) tensor(9) tensor(13) tensor(17) tensor(6) tensor(18) tensor(12) tensor(9) tensor(9) tensor(6) tensor(10) tensor(18) tensor(9) tensor(9) tensor(12) tensor(15) tensor(19) tensor(4) tensor(18) tensor(15)\n",
            "r n b 1 k 3 - p 2 p 1 p q 1 - 4 p 2 r - 1 p 2 P 3 - 2 p P 2 n 1 - 2 P B 1 N 2 - P P N Q 2 P P - R 3 K 2 R\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch.nn.functional as F\n",
        "class EncoderCNN(nn.Module):\n",
        "    def __init__(self, model_name, embed_size):\n",
        "   \n",
        "        super(EncoderCNN, self).__init__()\n",
        "        \n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2) #64\n",
        "        )\n",
        "        \n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2) #32\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(),\n",
        "           nn.MaxPool2d(2) #16\n",
        "        )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(),\n",
        "           nn.MaxPool2d(2) #8\n",
        "        )\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2)  #4\n",
        "        )\n",
        "        \n",
        "       \n",
        "        self.linear = nn.Linear(4 * 4 * 256, embed_size)\n",
        "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "        out = self.conv4(out)\n",
        "        out = self.conv5(out)\n",
        "        out = out.view(out.size(0), -1)  # flatten\n",
        "        out = F.relu(self.linear(out))\n",
        "        out = self.bn(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "    # def forward(self, x):\n",
        "    #     x = self.conv1(x)\n",
        "    #     x = nn.ReLU()(x)\n",
        "    #     x = self.pool(x)\n",
        "    #     x = self.conv2(x)\n",
        "    #     x = nn.ReLU()(x)\n",
        "    #     x = self.pool(x)\n",
        "    #     x = x.view(x.size(0), -1)\n",
        "    #     x = self.linear(x)\n",
        "    #     x = nn.ReLU()(x)\n",
        "    #     x = self.bn(x)\n",
        "       \n",
        "    #     return x\n",
        "                    "
      ],
      "metadata": {
        "id": "ZoL1-hpfzAAm"
      },
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout, tie_weights):\n",
        "        super(DecoderLSTM, self).__init__()\n",
        "        \n",
        "        if tie_weights:\n",
        "            embed_size = hidden_size\n",
        "            \n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True, dropout=0.35)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        \n",
        "        if tie_weights:\n",
        "            # share weights between embedding and classification layer\n",
        "            self.fc.weight = self.embedding.weight\n",
        "            \n",
        "        self.init_weights()\n",
        "        \n",
        "    def init_weights(self):\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        \n",
        "    def forward(self, features, captions, lengths):\n",
        "        x = self.embedding(captions)\n",
        "        x = torch.cat([features.unsqueeze(1), x], dim=1)\n",
        "        x = self.dropout(x)\n",
        "        x = pack_padded_sequence(x, lengths, batch_first=True)\n",
        "        x, _ = self.lstm(x)\n",
        "        x = self.dropout(x[0])\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "    \n",
        "    def sample(self, features, states=None):\n",
        "        \"\"\"Samples captions for given image features (Greedy search).\"\"\"\n",
        "        sampled_ids = []\n",
        "        inputs = features.unsqueeze(1)\n",
        "\n",
        "        for i in range(64):                                      # maximum sampling length\n",
        "            hiddens, states = self.lstm(inputs, states)          # (batch_size, 1, hidden_size), \n",
        "            outputs = self.fc(hiddens.squeeze(1))                # (batch_size, vocab_size)\n",
        "            token_id = outputs.max(1)[1]\n",
        "            sampled_ids += [token_id]\n",
        "            inputs = self.embedding(token_id)\n",
        "            inputs = inputs.unsqueeze(1)                         # (batch_size, 1, embed_size)\n",
        "        sampled_ids = torch.cat(sampled_ids, 0)                  # (batch_size, 20)\n",
        "        return sampled_ids.squeeze()"
      ],
      "metadata": {
        "id": "Q5oDeKh3zDR9"
      },
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "    \n",
        "    def __init__(self, cnn_name, vocab_size, embed_size, hidden_size, num_layers, dropout, tie_weights):\n",
        "        super(EncoderDecoder, self).__init__()\n",
        "        \n",
        "        if tie_weights:\n",
        "            embed_size = hidden_size\n",
        "            \n",
        "        self.encoder = EncoderCNN(cnn_name, embed_size)\n",
        "        self.decoder = DecoderLSTM(vocab_size, embed_size, hidden_size, num_layers, dropout, tie_weights)\n",
        "        \n",
        "        # create output folder to save weights\n",
        "        self.save_path = f'{cnn_name}-{embed_size}-{hidden_size}-{num_layers}'\n",
        "        if not os.path.exists(self.save_path):\n",
        "            os.mkdir(self.save_path)\n",
        "    \n",
        "    def forward(self, images, captions, lengths):\n",
        "        features = self.encoder(images)\n",
        "        outputs = self.decoder(features, captions, lengths)\n",
        "        return outputs\n",
        "    \n",
        "    def save(self, epoch, loss):\n",
        "        torch.save({'encoder': self.encoder.state_dict(), \n",
        "                    'decoder': self.decoder.state_dict()}, f'{self.save_path}/{epoch}-{loss:.2f}.pth')\n",
        "    \n",
        "    def load(self, epoch):\n",
        "        model_path = glob(f'{self.save_path}/{epoch}-*.pth')[-1]\n",
        "        try:\n",
        "            d = torch.load(model_path)\n",
        "            self.encoder.load_state_dict(d['encoder'])\n",
        "            self.decoder.load_state_dict(d['decoder'])\n",
        "        except:\n",
        "            print('Invalid epoch number <{}>, the model does not exist!'.format(epoch))"
      ],
      "metadata": {
        "id": "2YGJykt_zFEc"
      },
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model hyper-parameters\n",
        "cnn_name = 'resnet101'\n",
        "embed_size  = 1024\n",
        "hidden_size = 1024\n",
        "num_layers  = 2\n",
        "tie_weights = True\n",
        "\n",
        "# training hyper-parameters\n",
        "start_epoch = 0\n",
        "num_epochs  = 20\n",
        "learning_rate = 0.001"
      ],
      "metadata": {
        "id": "Q-4U9sJvzIRk"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "To_7vJmxzKgt"
      },
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_epoch(model, train_dl, criterion, optimizer, scheduler, epoch, last_epoch):\n",
        "    model.encoder.train()\n",
        "    model.decoder.train()\n",
        "    scheduler.step()\n",
        "    \n",
        "    total_steps = len(train_dl)\n",
        "    epoch_loss = 0.0\n",
        "    \n",
        "    for i, (images, captions, lengths) in enumerate(train_dl):\n",
        "        images, captions = images.to(device), captions.to(device)\n",
        "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
        "        \n",
        "        # forward step\n",
        "        outputs = model(images, captions, lengths)\n",
        "        loss = criterion(outputs, targets)\n",
        "        epoch_loss = (epoch_loss * i + loss.item()) / (i + 1)\n",
        "        \n",
        "        # backward step\n",
        "        model.encoder.zero_grad()\n",
        "        model.decoder.zero_grad()\n",
        "        \n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm(model.decoder.parameters(), 5.0)\n",
        "        optimizer.step()\n",
        "        \n",
        "        # report log info\n",
        "        sys.stdout.flush()\n",
        "        sys.stdout.write('\\rEpoch [%2d/%2d], Step [%3d/%3d], Loss = %.4f, Perplexity = %.4f    '\n",
        "                         % (epoch+1, last_epoch, i+1, total_steps, epoch_loss, np.exp(epoch_loss)))\n",
        "    print()\n",
        "\n",
        "    return epoch_loss\n",
        "\n",
        "\n",
        "def train(model, train_dl, criterion, optimizer, scheduler, start_epoch=0, num_epochs=10):\n",
        "    last_epoch = start_epoch + num_epochs\n",
        "        \n",
        "    for epoch in range(start_epoch, last_epoch):                \n",
        "        # train step\n",
        "        trn_loss = train_epoch(model, data_loader, criterion, optimizer, scheduler, epoch, last_epoch)\n",
        "        \n",
        "        # save model\n",
        "        model.save(epoch, trn_loss)"
      ],
      "metadata": {
        "id": "Cxzn8vQJzMpm"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = EncoderDecoder(cnn_name, len(vocab), embed_size, hidden_size, num_layers, 0.3, tie_weights)\n",
        "if use_gpu:\n",
        "    model = model.cuda()"
      ],
      "metadata": {
        "id": "dlkJpFW8zURF"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "use_gpu"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tA7_qs3G3VV0",
        "outputId": "ce1caa62-cd00-409a-d401-4e4f2e9b5963"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# loss function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "if use_gpu:\n",
        "    print('we')\n",
        "    criterion = criterion.cuda()\n",
        "  \n",
        "# list of parameters which will be updated\n",
        "params = list(model.decoder.parameters())\n",
        "params += list(model.encoder.parameters()) \n",
        "\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.RMSprop(params, lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.97)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhTm7j2HzWlw",
        "outputId": "e1080328-c937-4241-fad8-e3aee38f0dd3"
      },
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "we\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train(model, data_loader, criterion, optimizer, scheduler, start_epoch, num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Ym_noTgzYkW",
        "outputId": "7ad0f09d-ec94-4244-f2c0-7ad28fdd0b92"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\rEpoch [ 1/10], Step [  1/1126], Loss = 0.1265, Perplexity = 1.1349    "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-124-9ccd963b1ee5>:23: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
            "  torch.nn.utils.clip_grad_norm(model.decoder.parameters(), 5.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [ 1/10], Step [1126/1126], Loss = 0.1693, Perplexity = 1.1844    \n",
            "Epoch [ 2/10], Step [1126/1126], Loss = 0.1636, Perplexity = 1.1777    \n",
            "Epoch [ 3/10], Step [1126/1126], Loss = 0.1568, Perplexity = 1.1697    \n",
            "Epoch [ 4/10], Step [1126/1126], Loss = 0.1502, Perplexity = 1.1621    \n",
            "Epoch [ 5/10], Step [1126/1126], Loss = 0.1444, Perplexity = 1.1553    \n",
            "Epoch [ 6/10], Step [1126/1126], Loss = 0.1396, Perplexity = 1.1499    \n",
            "Epoch [ 7/10], Step [1126/1126], Loss = 0.1348, Perplexity = 1.1443    \n",
            "Epoch [ 8/10], Step [1126/1126], Loss = 0.1281, Perplexity = 1.1366    \n",
            "Epoch [ 9/10], Step [1126/1126], Loss = 0.1240, Perplexity = 1.1321    \n",
            "Epoch [10/10], Step [1126/1126], Loss = 0.1195, Perplexity = 1.1270    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from PIL import Image\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.CenterCrop(crop_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406),\n",
        "                         (0.229, 0.224, 0.225))\n",
        "])"
      ],
      "metadata": {
        "id": "-klOixWWbhbB"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_caption(model, img_filenames):\n",
        "    model.encoder.eval()\n",
        "    model.decoder.eval()\n",
        "    \n",
        "    captions = []\n",
        "    \n",
        "    for img_filename in img_filenames:\n",
        "\n",
        "        # prepare test image\n",
        "        image = load_image(img_filename, val_transform)\n",
        "        image_tensor = image.to(device)\n",
        "\n",
        "        # Generate features from image\n",
        "        feature = model.encoder(image_tensor)\n",
        "\n",
        "        # Generate caption from image\n",
        "        sampled_ids = model.decoder.sample(feature)\n",
        "        sampled_ids = sampled_ids.cpu().data.numpy()\n",
        "\n",
        "        # decode word ids to words\n",
        "        sampled_caption = []\n",
        "        for word_id in sampled_ids:\n",
        "            word = vocab.idx2word[word_id]\n",
        "            if word == '<EOS>': break\n",
        "            sampled_caption.append(word)\n",
        "\n",
        "        caption = \"\".join(sampled_caption[1:])\n",
        "        captions.append((img_filename, caption))\n",
        "    \n",
        "    return captions"
      ],
      "metadata": {
        "id": "KXw3yzOZbj7M"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_filenames = glob('/content/Problem03/train/*.png')[:10] \n",
        "captions = generate_caption(model, img_filenames)\n",
        "\n",
        "for img, caption in captions:\n",
        "    display((caption, img))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "A3A7NLaobmx1",
        "outputId": "4d4cad98-a5e3-42e3-8b85-829fc66a59da"
      },
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('8 - 1 p p 1 p p 2 - p 7 - 3 P k 3 - 1 P 5 P - P 5 R 1 - 6 P 1 - 6 K 1',\n",
              " '/content/Problem03/train/8-1pp1pp2-p7-3Pk3-1P5P-P5R1-6P1-6K1.png')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('r 1 b 1 k 2 r - 5 p p p - p 2 q p 3 - 3 p 4 - 8 - 5 N 2 - P P 3 P P P - R 2 Q R 1 K 1',\n",
              " '/content/Problem03/train/r1b1k2r-5ppp-p2qp3-3p4-8-5N2-PP3PPP-R2QR1K1.png')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('r 4 r k 1 - p p q 2 p p p - 5 n 2 - 3 P 1 P 2 - 3 P 1 b 2 - 1 B 5 Q - P P 2 p 1 P P - R N 2 R 1 K 1',\n",
              " '/content/Problem03/train/r4rk1-ppq2ppp-5n2-3P1P2-3P1b2-1B5Q-PP2p1PP-RN2R1K1.png')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('1 k 2 r 3 - p p p 5 - 3 p N 3 - 1 P 1 n p 2 p - P 5 p P - B 1 b 2 r P 1 - 2 R 4 K - 2 R 5',\n",
              " '/content/Problem03/train/1k2r3-ppp5-3pN3-1P1np2p-P5pP-B1b2rP1-2R4K-2R5.png')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('6 k 1 - p 1 p 3 p 1 - 1 p 4 p 1 - 4 P 3 - 5 P 2 - 1 P P 5 - P 6 P - R 5 K 1',\n",
              " '/content/Problem03/train/6k1-p1p3p1-1p4p1-4P3-5P2-1PP5-P6P-R5K1.png')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('8 - 4 b 1 k p - 5 p 2 - 4 p P p 1 - 1 P p 1 P 1 P 1 - n 1 P 1 B 2 P - Q 3 B K 2 - 7 q',\n",
              " '/content/Problem03/train/8-4b1kp-5p2-4pPp1-1Pp1P1P1-n1P1B2P-Q3BK2-7q.png')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('r r 4 k 1 - p b p q 1 p b p - 6 p 1 - 2 P p 2 n 1 - 1 P 1 P 4 - 2 N 1 P P 2 - P B Q 3 P P - R 1 B 1 R 1 K 1',\n",
              " '/content/Problem03/train/rr4k1-pbpq1pbp-6p1-2Pp2n1-1N1P4-2N1PP2-PPQ3PP-R1B1R1K1.png')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('3 R 4 - 8 - 2 r 3 k p - 5 p p 1 - 8 - 5 P K 1 - 8 - 8',\n",
              " '/content/Problem03/train/3R4-8-2r3kp-5pp1-8-5PK1-8-8.png')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('2 r 1 k 2 r - 3 b 1 p p p - n p n N p 3 - p 2 p 4 - Q 2 P 4 - 4 P 3 - P P 1 B 1 P P P - 2 R 2 R K 1',\n",
              " '/content/Problem03/train/2r1k2r-3b1ppp-Bpnqp3-p2p4-Q2P4-4P3-PP1N1PPP-2R2RK1.png')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('r n b 1 k 1 n r - p p p p q p p p - 8 - 8 - 1 b B 1 P 3 - 2 N 2 N 2 - P P Q 2 P P P - R 1 B 1 K 2 R',\n",
              " '/content/Problem03/train/rnb1k1nr-ppppqppp-8-8-1bB1P3-2N2N2-PPQ2PPP-R1B1K2R.png')"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img_filenames = glob('/content/Problem03/test/*.png')[:10] \n",
        "captions = generate_caption(model, img_filenames)\n",
        "\n",
        "for img, caption in captions:\n",
        "    display((caption, img))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "ocWmF0z5chFc",
        "outputId": "8a7a8193-8076-49b5-dda6-5e3b65a0ea62"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('r 1 r 3 k 1 - 3 p 1 p p p - 1 q 2 p n 2 - p N n 5 - P p 6 - 5 P P B - 2 Q 1 P 2 P - R 3 K 2 R',\n",
              " '/content/Problem03/test/img3605.png')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('4 r k 2 - 5 p p p - 8 - 3 p 4 - 3 P 4 - 5 P 1 P - 2 Q 2 P P K - 4 q 3',\n",
              " '/content/Problem03/test/img1659.png')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('r n 2 r 1 k 1 - 2 q 1 b 1 p p - 3 p 4 - 1 p 1 P p 3 - 8 - P 2 B B 3 - 1 P 3 Q P P - 2 R 2 R 1 K',\n",
              " '/content/Problem03/test/img3566.png')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('r 1 b q k 2 r - 1 p p 1 n p p 1 - p 3 p 2 p - 4 b 3 - 2 B 5 - 6 Q P - P P P P 1 P P 1 - R N B 1 R 1 K 1',\n",
              " '/content/Problem03/test/img3573.png')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('r 2 q 1 r 1 k - p p b 1 b n p p - 5 p 2 - 1 P 1 N p 3 - P 1 B p P 3 - 1 Q 1 P 1 N 2 - 5 P P P - R 4 R K 1',\n",
              " '/content/Problem03/test/img3668.png')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('8 - 4 P 3 - 2 K B 1 k 2 - 3 Q 1 P 2 - 8 - 4 b 3 - 8 - 8',\n",
              " '/content/Problem03/test/img4338.png')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('r 2 q k 2 r - p 2 n b p 2 - 3 p 1 n 1 p - 1 Q 2 p 1 p 1 - 4 P 3 - 2 P P B N 1 P - P P 1 N 1 P P 1 - R 4 R K 1',\n",
              " '/content/Problem03/test/img65.png')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('r 5 k 1 - 1 b q 2 p p p - 4 p 3 - p 3 N 3 - 1 P B 5 - 4 P 3 - 5 P P P - R Q 4 K 1',\n",
              " '/content/Problem03/test/img3246.png')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('r 1 b q k b 1 r - p p 3 p p p - 2 n 2 n 2 - 3 p p 3 - 8 - 1 Q P P 4 - P P 3 P P P - R N B 1 K B N R',\n",
              " '/content/Problem03/test/img105.png')"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "('r n 6 - 4 k p 2 - 2 p 3 p 1 - 7 p - p 1 B 1 P 1 b 1 - P 1 P 2 N P 1 - 2 P 5 - 3 R 2 K 1',\n",
              " '/content/Problem03/test/img1425.png')"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "\n",
        "with open('predictions.csv', 'w', newline='') as file:\n",
        "     writer = csv.writer(file)\n",
        "     for i in range(4562) :\n",
        "       img_filenames = glob(f'/content/Problem03/test/img{i}.png')\n",
        "       captions = generate_caption(model, img_filenames)\n",
        "       for img, caption in captions:\n",
        "         \n",
        "            writer.writerow([f\"img{i}\",caption])\n",
        "\n",
        "                \n",
        "    "
      ],
      "metadata": {
        "id": "3Nyys0PWfRa0"
      },
      "execution_count": 136,
      "outputs": []
    }
  ]
}